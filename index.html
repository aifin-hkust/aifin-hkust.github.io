<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
   <META http-equiv=Content-Type content="text/html; charset=gb2312">
   <title>MAFS 5440: AI in Fintech </title>
</head>
<body background="../../images/crysback.jpg">

<!-- PAGE HEADER -->

<div class="Section1">
<table border="0" cellpadding="0" width="100%" style="width: 100%;">
      <tbody>
        <tr>

       <td style="padding: 0.75pt;" width="80" align="center">

      <p class="MsoNormal">&nbsp;<img width="64" height="64"
 id="_x0000_i1025"
 src="../../images/hkust0_starry.jpg" alt="PKU">
          </p>
       </td>
       <td style="padding: 0.75pt;">
      <p>
<span style="font-size: 18pt;">
<b><big>MAFS 5440. Artificial Intelligence in Fintech <br>
   Fall 2025 </big></b>
<br>
</p>
</td>
</tr>

</tbody>
</table>

<div class="MsoNormal" align="center" style="text-align: center;">
<hr size="2" width="100%" align="center">  </div>

<ul type="disc">

</ul>

<!-- COURSE INFORMATION BANNER -->

<table border="0" cellpadding="0" width="100%" bgcolor="#990000"
 style="background: rgb(153,0,0) none repeat scroll 0% 50%; width: 100%;">
      <tbody>

        <tr>
       <td style="padding: 2.25pt;">
      <p class="MsoNormal"><b><span
 style="font-size: 13.5pt; color: white;">Course Information</span></b></p>
       </td>
      </tr>

  </tbody>
</table>

<!-- COURSE INFORMATION -->

<h3>Synopsis</h3>
<p style="margin-left: 0.5in;">
<big> This course offers a comprehensive exploration of the fundamental concepts and underlying principles of artificial intelligence (AI). It delves into the core principles of machine learning and provides valuable insights through case studies of relevant technologies. By providing opportunities for hands-on experimentation with machine learning applications, the course aims to inspire students to devise innovative approaches to address real-life problems in fintech using readily-available AI technologies. 	</big>
<br>
	<big>
		<b>Prerequisite</b>: Some preliminary course on (statistical) machine learning, applied statistics, and deep learning will be helpful.
	</big>
</p>


<h3>Instructors: </h3>
		
<p style="margin-left: 0.5in;">
<big>
<em><a href="http://yao-lab.github.io/">Yuan Yao</a>  </em>
</big>
</p>

<h3>Time and Place:</h3>
<p style="margin-left: 0.5in;">
<big><em>Wednesday 19:30-22:20pm, G010, CYT Bldg (140)</em> <br>
</big>
</p>
	
<h3>Reference (&#21442;&#32771;&#25945;&#26448;)</h3>

	

	<p style="margin-left: 0.5in;">

	<big>

		<em> <a href="https://www.statlearning.com/"> An Introduction to Statistical Learning, with applications in R / Python.</a> By James, Witten, Hastie, and Tibshirani </em>

	</big>

	</p>

	

	<p style="margin-left: 0.5in;">

	<big>

		<em> <a href="https://github.com/JWarmenhoven/ISLR-python/">ISLR-python, By Jordi Warmenhoven</a>. </em>

	</big>

	</p>

	

	<p style="margin-left: 0.5in;">

	<big>

		<em> <a href="https://github.com/mscaudill/IntroStatLearn">ISLR-Python: Labs and Applied, by Matt Caudill</a>. </em>

	</big>

	</p>

	<p style="margin-left: 0.5in;">
<big>
<em><a href="https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff">Manning: Deep Learning with Python</a>, by Francois Chollet</em> [<a href="https://github.com/fchollet/deep-learning-with-python-notebooks">GitHub source in Python 3.6 and Keras 2.0.8</a>]
</big>
</p>

	<p style="margin-left: 0.5in;">
<big>
<em><a href="http://www.deeplearningbook.org/">MIT: Deep Learning</a>, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
</em>
</big>
</p>

	

<h3>Tutorials: preparation for beginners</h3>
<p style="margin-left: 0.5in;">
<big>
	<em><a href="http://cs231n.github.io/python-numpy-tutorial/">Python-Numpy Tutorials</a> by Justin Johnson </em>
</big>
</p>

<p style="margin-left: 0.5in;">
<big>
	<em><a href="http://scikit-learn.org/stable/tutorial/">scikit-learn Tutorials</a>: An Introduction of Machine Learning in Python</em>
</big>
</p>	

<p style="margin-left: 0.5in;">
<big>
	<em><a href="http://cs231n.github.io/ipython-tutorial/">Jupyter Notebook Tutorials</a> </em>
</big>
</p>
	
<p style="margin-left: 0.5in;">
<big>
<em><a href="http://pytorch.org/tutorials/">PyTorch Tutorials</a> </em>
</big>
</p>
<p style="margin-left: 0.5in;">
<big>
<em><a href="http://www.di.ens.fr/~lelarge/dldiy/">Deep Learning: Do-it-yourself with PyTorch</a>, </em> A course at ENS
</big>
</p>
<p style="margin-left: 0.5in;">
<big>
<em><a href="https://www.tensorflow.org/tutorials/">Tensorflow Tutorials</a></em>
</big>
</p>
<p style="margin-left: 0.5in;">
<big>
<em><a href="https://mxnet.incubator.apache.org/tutorials/index.html">MXNet Tutorials</a></em>
</big>
</p>
<p style="margin-left: 0.5in;">
<big>
<em><a href="http://deeplearning.net/software/theano/tutorial/">Theano Tutorials</a></em>
</big>
</p>	


<p style="margin-left: 0.5in;">

<big>

<em> <a href="http://www-stat.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning (ESL).</a> 2nd Ed. By Hastie, Tibshirani, and Friedman </em>

</big>

</p>

	

	<p style="margin-left: 0.5in;">

	<big>

	<em> <a href="https://github.com/sujitpal/statlearning-notebooks">statlearning-notebooks</a>, by Sujit Pal, Python implementations of the R labs for the <a href="https://lagunita.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/about">StatLearning: Statistical Learning</a> online course from Stanford taught by Profs Trevor Hastie and Rob Tibshirani.</em>

	</big>

	</p>


<h3>Teaching Assistant:</h3>

<p style="margin-left: 0.5in;">
<big> <br>
Email: Mr. FU, Xiaoyi < <em> aifin.hkust (add "AT gmail DOT com" afterwards) </em> >
</big>
</p>
	
				
<h3>Schedule</h3>

<table border="1" cellspacing="0">
<tbody>

<tr>
<td align="left"><strong>Date</strong></td>
<td align="left"><strong>Topic</strong></td>
<td align="left"><strong>Instructor</strong></td>
<td align="left"><strong>Scriber</strong></td>
</tr>

<tr>
<td>03/09/2025, Wed </td>
<td>Lecture 01: Overview and History of Artificial Intelligence in Fintech. [<a href="slides/Lecture01_overview.pdf"> slides </a>]
		<!---
	<ul>[ Project 1 ]
		<li> Warm-up project description [<a href="project1/project1.pdf"> pdf </a>] </li>
		<li> Kaggle: Home Credit Default Risk [<a href="https://www.kaggle.com/c/home-credit-default-risk"> link </a>]
		</li>
		
	
		<li> GitHub Repository for reports of Project 1
		<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/project1//"> [ GitHub ] </a>
		<p>
		</li>	
		
	</ul>
	--->

</td>
<td>Y.Y.</td>
<td></td>
</tr>
	

	

	<tr>
<td>10/09/2025, Wed </td>
<td>Lecture 02: Supervised Learning: Linear Regression and Classification <a href="slides/Lecture02_linear.pdf">[ slides ]</a>
	
	<ul>[ Reference ]
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Linear Regression Python Notebook <a href="notebook/MAFS6010_Regression.ipynb"> [ MAFS6010_Regression.ipynb ] </a> </li> 
		<li> Linear Classification Python Notebook <a href="notebook/MAFS6010_Classification.ipynb"> [ MAFS6010_Classification.ipynb ] </a> </li> 
 	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>		

		<tr>
<td>11/09/2025, Thu </td>
<td> Seminar.
		<ul>
		<li> <B>Title</B>: PKU Quest: AI-Powered Math Education Practice at Peking University [<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/course/statml/slides/Seminar_ChenL-LiuZ.png"> announcement </a>] [<a href=""> slides </a>]</li>
		<li> <B>Speaker</B>: Leheng Chen and Zihao Liu, Peking University </li>
		<li> <B>Time</B>: Thursday Sep 11, 2025, 3:30pm </li>
		<li> <B>Venue</B>: Room 2612B (near Lift 31 & 32) </li>
		<li> <B>Abstract</B>: 
			The advent of Generative AI necessitates a paradigm shift in higher education, calling for new, diverse models of interaction between students, teachers, and AI. In response to this challenge, Peking University has developed PKU Quest, an AI-assisted platform designed to explore these new pedagogical frontiers. PKU Quest focuses on optimizing for the unique demands of mathematics education, and has developed the "Math Tutor," a tool specifically designed for math problem-solving support. Instead of providing direct answers, the Math Tutor engages students in a heuristic and exploratory dialogue, guiding them to develop independent thinking and problem-solving skills. This application has now been implemented across all foundational mathematics courses at Peking University. 
			This presentation will share our journey in developing PKU Quest, discussing the motivations, challenges, and practical outcomes of what we consider a first step in exploring the vast potential of AI in education.
		<li> <B>Bio</B>: 
			<I>Leheng Chen</I> is a Ph.D. student at the Beijing International Center for Mathematical Research (BICMR), Peking University, advised by Professor Bin Dong. He has broad interests in the application of artificial intelligence. Previously, he explored research directions in AI for Science, such as thermodynamic modeling and foundation models for partial differential equations, with his work published in Physical Review E and at an ICLR Workshop. He has since shifted his research focus to the practical application of AI in Education, where he designed and developed "PKU Quest," an AI-assisted teaching and learning platform for Peking University.
			<br>
			<I>Zihao Liu (Leo)</I> is a Ph.D. student in Applied Mathematics and Artificial Intelligence at the School of Mathematical Sciences, Peking University. His interests span the application of AI to education and scientific understanding, with recent work focusing on improving the pedagogical effectiveness of AI-powered educational agents and building benchmark datasets for evaluating AI capabilities. As the founder and lead developer of PKU Quest and AKIS (AI Knowledge Intelligent Solution), he focuses on the practical deployment of AI-in-education systems and has helped design and develop “AIBOOKS,” an intelligent digital-textbook platform, and “Math Tutor,” 
	a guided problem-solving assistant for students. He is deeply committed to advancing the integration of AI and education.
		</ul> 
</td>
<td>Y.Y.</td>
<td></td>
</tr>		

	<tr>
<td>17/09/2025, Wed </td>
<td>Lecture 03: Model Assessment and Selection: Subset, Ridge, Lasso, and PCR <a href="slides/Lecture03_selection.pdf">[ slides ]</a> 
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Python Notebook for Model Selection (Subset, Ridge, Lasso, and Principal Component Regression) 
			<a href="notebook/MAFS6010_Selection.ipynb"> [ MAFS6010_Selection.ipynb ] </a> </li> 
	</ul>
	<ul>[ Seminar ] 
		<li> <B>Speaker</B>: QRT guest speakers [<a href="slides/QRT-oncampus.jpeg"> poster </a>]</li>
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>

	<tr>
<td>24/09/2025, Wed </td>
<td>Lecture 04: Project 1  (via Canvas Zoom due to typhoon) <a href="2025/project1.pdf">[ pdf ]</a>   
	<br>
	<ul>[ Reference ]:
		<li> Kaggle: Home Credit Default Risk [<a href="https://www.kaggle.com/c/home-credit-default-risk"> link </a>]
		</li>
		<li> Kaggle: M5 Forecasting - Accuracy, Estimate the unit sales of Walmart retail goods. 
		[<a href="https://www.kaggle.com/c/m5-forecasting-accuracy"> link </a>]
		</li>
	</ul>
	
</td>
<td>Y.Y.</td>
<td></td>
</tr>

	<tr>
<td>27/09/2025, Sat</td>
<td>Lecture 05: Decision Tree, Bagging, Random Forests and Boosting <a href="slides/Lecture05_tree.pdf">[ slides ]</a> 
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Python Notebook for Decision Trees, Bagging, Random Forests and Boosting 
			<a href="notebook/MAFS6010_tree.ipynb"> [ MAFS6010_tree.ipynb ] </a> </li> 
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>

	<tr>
<td>10/08/2025, Wed</td>
<td>Lecture 06: Support Vector Machines [<a href="slides/Lecture06_svm.pdf"> slides </a>]   
	<br>
	<ul>[ Reference ]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Python Notebook for Support Vector Machines 
			<a href="notebook/MAFS6010_svm.ipynb"> [ MAFS6010_svm.ipynb ] </a> </li> 
		<br>
		<li> Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro. <B>The Implicit Bias of Gradient Descent on Separable Data.</B> 
		[<a href="https://arxiv.org/abs/1710.10345"> arXiv:1710.10345 </a>]. ICLR 2018. Gradient descent on logistic regression leads to max margin. </li>
		<li> Matus Telgarsky. <B>Margins, Shrinkage, and Boosting.</B> <a href="https://arxiv.org/abs/1303.4172">[ arXiv:1303.4172 ]</a>. ICML 2013. An older paper on gradient descent on exponential/logistic loss 
		leads to max margin. </li>
		<li> Yuan Yao, <a href="http://www.disi.unige.it/person/RosascoL/">Lorenzo Rosasco</a> and <a href="http://www.pascal-network.org/Network/Researchers/150/">Andrea Caponnetto</a>. <B>On Early Stopping in Gradient Descent Learning.</B> <i>Constructive Approximation</i>, 2007, 26 (2): 289-315.
[<a href="https://link.springer.com/article/10.1007/s00365-006-0663-2"> link </a>]
</li>
		<li> Jingfeng Wu, Peter L. Bartlett, Jason D. Lee, Sham M. Kakade, and Bin Yu. <B>Risk Comparisons in Linear Regression: Implicit Regularization Dominates Explicit Regularization.</B> <a href="https://arxiv.org/abs/2509.17251">[ arXiv:2509.17251 ]</a></li>
	</ul>

</td>
<td>Y.Y.</td>
<td></td>
</tr>

		<tr>
	<td>10/11/2025, Sat </td>
	<td> Lecture 6+: Seminar </a> 
	<br>

	<ul>
		<li> <B>Title</B>: A Statistical View on Implicit Regularization: Gradient Descent Dominates Ridge [<a href="https://yao-lab.github.io/course/statml/slides/Seminar_wu2025risk_slides.pdf"> slides </a>]</li>
		<li> <B>Speaker</B>: Dr. <a href="https://uuujf.github.io/">Jingfeng WU</a>, UC Berkeley </li>
		<li> <B>Time</B>: 10:30am, LTF </li>
		<li> <B>Abstract</B>: A key puzzle in deep learning is how simple gradient methods find generalizable solutions without explicit regularization. This talk discusses the implicit regularization of gradient descent (GD) through the lens of statistical dominance. Using linear regression as a clean proxy, we present three surprising findings.

First, GD dominates ridge regression: with comparable regularization, the excess risk of GD is always within a constant factor of ridge, but ridge can be polynomially worse even when tuned optimally. Second, GD is incomparable with online stochastic gradient descent (SGD). While it is known that for certain problems GD can be polynomially better than SGD, the reverse is also true: we construct problems, inspired by benign overfitting theory, where optimally stopped GD is polynomially worse. Finally, GD dominates SGD for a significant subclass of problems -- those with fast and continuously decaying covariance spectra -- which includes all problems satisfying the standard capacity condition.

This is joint work with Peter Bartlett, Sham Kakade, Jason Lee, and Bin Yu.
		<li> <B>Bio</B>: 
Jingfeng Wu is a postdoctoral fellow at the Simons Institute for the Theory of Computing at UC Berkeley. His research focuses on deep learning theory, optimization, and statistical learning. He earned his Ph.D. in Computer Science from Johns Hopkins University in 2023. Prior to that, he received a B.S. in Mathematics (2016) and an M.S. in Applied Mathematics (2019), both from Peking University. In 2023, he was recognized as a Rising Star in Data Science by the University of Chicago and UC San Diego.
		</li>
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>

	<tr>
<td>10/15/2025, Wed</td>
<td>Lecture 07: An Introduction to Convolutional Neural Networks [<a href="slides/Lecture07_CNN.pdf"> slides </a>] and <a href="https://easychair.org">EasyChair</a> Instruction for Project 1 [<a href="project1/Project1_MAFS5440_Instruction.pdf"> slides </a>] 
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> LeNet5 for MNIST dataset in Pytorch Notebook 
			[<a href="https://aifin-hkust.github.io/notebook/LeNet5_mnist.ipynb"> LeNet5_mnist.ipynb </a>] </li>
		<li> LeNet5 for Cifar10 dataset in Pytorch Notebook 
			[<a href="https://aifin-hkust.github.io/notebook/LeNet5_cifar10.ipynb"> LeNet5_cifar10.ipynb </a>] </li>
		<li> AlexNet for Cifar10 dataset in Pytorch Notebook 
			[<a href="https://aifin-hkust.github.io/notebook/AlexNet_cifar10.ipynb"> AlexNet_cifar10.ipynb </a>] </li>
		<li> Original AlexNet source codes in Computer History Museum [<a href="https://github.com/computerhistory/AlexNet-Source-Code"> github </a>] </li>
		<li> ResNet for Cifar10 dataset in Pytorch Notebook 
			[<a href="https://aifin-hkust.github.io/notebook/ResNet_cifar10.ipynb"> ResNet_cifar10.ipynb </a>] </li>
		<li> Fine-tuning (transfer learning) of ResNet in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/finetuning_resnet.ipynb"> finetuning_resnet.ipynb </a>] </li>
		<li> Visualization of VGG16 in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/vgg16-visualization.ipynb"> vgg16-visualization.ipynb </a>] </li>
		<li> Class activation heatmap of VGG16 in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/vgg16-heatmap.ipynb"> vgg16-heatmap.ipynb </a>] </li>
		<li> Neural Style of HKUST at Starry Night in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/neural_style_starry-hkust.ipynb"> neural_style_starry-hkust.ipynb </a>] </li>
		<li> Adversarial examples of LeNet5 with MNIST 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/LeNet5_mnist_fgsm.ipynb"> LeNet5_mnist_fgsm.ipynb </a>] </li>
	</ul>

</td>
<td>Y.Y.</td>
<td></td>
</tr>

	<tr>
<td>11/01/2025, Sat</td>
<td>Lecture 08: An Introduction to Recurrent Neural Networks (RNN), Long Short Term Memory (LSTM), Attention and Transformer [<a href="slides/Lecture08_RNN-LSTM-Transformer.pdf"> slides </a>], Rm 5560, 14:00.  
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Character-level RNN, LSTM and GRU for Name Classification 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/char_rnn_classification_tutorial.ipynb"> char_rnn_classification_tutorial.ipynb </a>] 
		</li>
		<li> RNN for generating Shakespeare's Sonnet 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/rnn.ipynb"> rnn.ipynb </a>] 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/shakespeare.txt"> shakespeare.txt </a>] 
		</li> 
		<li> LSTM for generating Shakespeare's Sonnet 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/rnn_lstm_shakespeare.ipynb"> rnn_lstm_shakespeare.ipynb </a>] 
		</li>	
		<li> Generating Shakespeare's Sonnet: RNN, LSTM, Bidirectional LSTM, and Momentum-LSTM
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/rnn_lstm_BiLSTM_mlstm_shakespeare.ipynb"> rnn_lstm_BiLSTM_mlstm_shakespeare.ipynb </a>]
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/rnn_lstm_biLSTM_shakespeare.ipynb"> rnn_lstm_biLSTM_shakespeare.ipynb </a>] 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/shakespeare.txt"> shakespeare.txt </a>]
		</li>
		<li> Bidirectional RNN for MNIST in pytorch
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/bidirection_lstm_mnist.ipynb"> bidirection_lstm_mnist.ipynb </a>] 
		[<a href="https://www.youtube.com/watch?v=jGst43P-TJA"> Youtube </a>]
		</li>	
		<li> Illustrated Transformer by Jay Alammar: [<a href="https://jalammar.github.io/illustrated-transformer/"> link </a>] </li>
		<li> The Annotated Transformer Tutorial by Sasha Rush: [<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html"> link </a>] </li>
		<p>
		<li> <a href="https://www.nobelprize.org/prizes/physics/2024/press-release/"> Nobel Prize in Physics 2024 </a> </li>
			<li> <a href="https://x.com/schmidhuberai/status/1844022724328394780?s=46&t=Eqe0JRFwCu11ghm5ZqO9xQ"> Jorgen Schmidhuber's Critique: </a>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The <a href="https://twitter.com/hashtag/NobelPrizeinPhysics2024?src=hash&amp;ref_src=twsrc%5Etfw">#NobelPrizeinPhysics2024</a> for Hopfield &amp; Hinton rewards plagiarism and incorrect attribution in computer science. It&#39;s mostly about Amari&#39;s &quot;Hopfield network&quot; and the &quot;Boltzmann Machine.&quot; <br><br>1. The Lenz-Ising recurrent architecture with neuron-like elements was published in…</p>&mdash; Jürgen Schmidhuber (@SchmidhuberAI) <a href="https://twitter.com/SchmidhuberAI/status/1844022724328394780?ref_src=twsrc%5Etfw">October 9, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
			</li>
		<li> <a href="https://x.com/hardmaru/status/1253189802452647936"> Geoff Hinton's response to Schmidhuber's critique: </a> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Geoff Hinton&#39;s response to Schmidhuber&#39;s critique on r/ML <a href="https://t.co/kUIznzbRBh">https://t.co/kUIznzbRBh</a> <a href="https://t.co/joNcuzJnFk">https://t.co/joNcuzJnFk</a> <a href="https://t.co/3HWbfT5T9F">pic.twitter.com/3HWbfT5T9F</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/1253189802452647936?ref_src=twsrc%5Etfw">April 23, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>		
		</li>
		<li> Jorgen Schmidhuber: [<a href="https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html"> Deep Learning: Our Miraculous Year 1990-1991
 </a>] [<a href="https://people.idsia.ch/~juergen/critique-honda-prize-hinton.html"> Critique of Honda Prize for Dr. Hinton </a>]			
		</li>	  
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>


	<tr>
<td>11/05/2025, Wed</td>
<td>Lecture 9: Transformer and Applications [<a href="slides/Lecture09_transformer.pdf"> slides </a>]  
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Pytorch Twitter Sentiment Analysis: RNN, LSTM, BiLSTM, Multihead Self-Attention. 
		[<a href="../notebook/PyTorch-RNN.ipynb"> RNN (ipynb) </a>] 
		[<a href="../notebook/PyTorch-LSTM.ipynb"> LSTM (ipynb) </a>] 
		[<a href="../notebook/PyTorch-BiLSTM.ipynb"> BiLSTM (ipynb) </a>] 
		[<a href="../notebook/PyTorch-BiLSTM-with-MHSA.ipynb"> BiLSTM with Multihead Attention (ipynb) </a>] 	
		[<a href="../notebook/PyTorch-BiLSTM-with-bertEmbedding.ipynb"> BERT embedding with BiLSTM (ipynb) </a>]
		[<a href="https://www.kaggle.com/kazanova/sentiment140"> Sentiment140 dataset </a>]
		</li>
		<li> Pytorch Sentiment Analysis with IMDB data: RNN, (bi)-LSTM, CNN, Transformer, BERT, etc. 
			[<a href="https://github.com/bentrevett/pytorch-sentiment-analysis"> GitHub </a>]
		</li>
		<li> Illustrated Transformer by Jay Alammar: [<a href="https://jalammar.github.io/illustrated-transformer/"> link </a>] </li>
		<li> The Annotated Transformer Tutorial by Sasha Rush: [<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html"> link </a>]
		</li>
		<li> BERT generation of Shakespeare's sonnet: [<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/data/BERT_shakespeare_gen.ipynb"> BERT_shakespeare_gen.ipynb </a>]
		</li>
		<li> BERT next sentence generation of Shakespeare's sonnet and Chinese poems: 
			[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/data/BERT_shakespeare_nextsen.ipynb"> BERT_shakespeare_nextsen.ipynb </a>]
		</li>
		<li> Chinese BERT (Whole-Word-Masking): [<a href="https://github.com/ymcui/Chinese-BERT-wwm"> link </a>]
		</li>
	</ul>	

	<ul> [ Seminar ]
		<li><B>Title</B>: Transformers As Statisticians: Provable In-Context Learning with In-Context Algorithm Selection. [<a href="slides/Seminar_Mei_transformer_statistician_slides.pdf"> slides </a>] [<a href="https://youtu.be/vKZ_I05sSj0?si=OSnLkWJP_oDlfcdg"> video </a>] </li>  
		<li> <B>Speaker</B>: Prof. <a href="https://www.stat.berkeley.edu/~songmei/">Song MEI</a>, University of California at Berkeley. </li>
		<li> <B>Abstract</B>: 
			Neural sequence models based on the transformer architecture have demonstrated remarkable <I>in-context learning</I> (ICL) abilities, where they can perform new tasks when prompted with training and test examples, without any parameter update to the model. 
			This work first provides a comprehensive statistical theory for transformers to perform ICL. Concretely, we show that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, 
			Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions. Using an efficient implementation of in-context gradient descent as the underlying mechanism, 
			our transformer constructions admit mild size bounds, and can be learned with polynomially many pretraining sequences.
Building on these ``base'' ICL algorithms, intriguingly, we show that transformers can implement more complex ICL procedures involving <I>in-context algorithm selection</I>, akin to what a statistician can do in real life -- A <I>single</I> transformer can adaptively select different base ICL algorithms -- 
			or even perform qualitatively different tasks -- on different input sequences, without any explicit prompting of the right algorithm or task. We both establish this in theory by explicit constructions, and also observe this phenomenon experimentally. 
			In theory, we construct two general mechanisms for algorithm selection with concrete examples: pre-ICL testing, and post-ICL validation. As an example, we use the post-ICL validation mechanism to construct a transformer that can perform nearly Bayes-optimal ICL on a challenging 
			task -- noisy linear models with mixed noise levels. Experimentally, we demonstrate the strong in-context algorithm selection capabilities of standard transformer architectures.		
		</li>
		<li> <B>Bio</B>: Song Mei is an Assistant Professor in the Department of Statistics and the Department of Electrical Engineering and Computer Sciences at UC Berkeley. In June 2020, he received Ph.D. from Stanford, with Prof. Andrea Montanari. 
			Song's research is motivated by data science and AI, and lies at the intersection of statistics, machine learning, information theory, and computer science. His current research interests include language models and diffusion models, theory of deep learning, 
			theory of reinforcement learning, high dimensional statistics, quantum algorithms, and uncertainty quantification. Song received Sloan Research Fellowship in 2025 and NSF career award in 2024.
		</li>
		<li> <B>Reference</B>: Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection. NeurIPS, 2023 (Oral). [<a href="https://arxiv.org/abs/2306.04637"> arXiv:2306.04637</a>]</li>
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>

	<tr>
<td>11/12/2025, Wed</td>
<td>Lecture 10: An Introduction to Reinforcement Learning with Applications in Quantitative Finance [<a href="slides/Lecture10_reinforcement.pdf"> slides </a>]  and Final Project Initialization  [<a href="project3/MAFS_5440_F25_project2.pdf"> project2.pdf </a>]
	<br>
	<ul>[ Reference ]:
		<li> Google DeepMind's Deep Q-learning playing Atari Breakout:
		[<a href="https://www.youtube.com/watch?v=V1eYniJ0Rnk"> youtube </a>]
		</li>
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Deep Q-Learning Pytorch Tutorial: [<a href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"> link </a> ]
		</li>
		<li> A Tutorial of Reinforcement Learning for Quantitative Trading: 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/tree/master/tutorial/Tutorial_FinRL_stock_trading_NeurIPS_2018_3run.ipynb"> Tutorial </a>]
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/tree/master/tutorial/Tutorial_FinRL_replicate_3run.ipynb"> Replicate </a>]
		</li>
		<li> FinRL: Deep Reinforcement Learning for Quantitative Finance
		[<a href="https://github.com/AI4Finance-Foundation/FinRL"> GitHub </a>]
		</li>
		<li> Reinforcement Learning and Supervised Learning for Quantitative Finance: [<a href="https://github.com/Ceruleanacg/Personae/blob/master/README.md"> link </a>]
		</li>
		<li> Hierarchical Reinforced Trader (HRT): A Bi-Level Approach for Optimizing Stock Selection and Execution, by Zijie Zhao, Roy E. Welsch. 
			[<a href="https://arxiv.org/abs/2410.14927"> arXiv:2410.14927 </a>]
		</li>
		<li> Prof. Michael Kearns, University of Pennsyvania, Algorithmic Trading and Machine Learning, 
			Simons Institute at Berkeley [<a href="https://simons.berkeley.edu/talks/michael-kearns-2015-11-19"> link </a> ]
		</li>
	</ul>
		
	<ul>[ Final Project ]		
		<li> Description: [<a href="project3/MAFS_5440_F25_project2.pdf"> project2.pdf </a>]
		</li>
		<!---
		<li> Kaggle Contests  [<a href="project3/project3_Crypto_M5.pdf"> slides </a>]
		</li>
		<li> Cryptocurrency Trading Project [<a href="project3/project3_CryptoTrading.pdf"> slides (pdf) </a>]
		</li>	
		--->
		<li> Large Language Models with Financial Analysis [<a href="project3/MAFS_5440_F25_Project2_LLM+finance.pdf"> slides (pdf) </a>] </li>
		<p>
		</ul>
	
	<ul>[ Reference ]
		<li> Kaggle: Home Credit Default Risk [<a href="https://www.kaggle.com/c/home-credit-default-risk"> link </a>]
		</li>
		<li> Kaggle: G-Research Crypto Forecasting. [<a href="https://www.kaggle.com/c/g-research-crypto-forecasting/"> link </a>]
		</li>
		<li> Kaggle: Jane Street Real-Time Market Data Forecasting. [<a href="https://www.kaggle.com/competitions/jane-street-real-time-market-data-forecasting/"> link </a>]
		</li> 
		<li> Kaggle: M5 Forecasting - Accuracy, Estimate the unit sales of Walmart retail goods. [<a href="https://www.kaggle.com/c/m5-forecasting-accuracy"> link </a>]
		</li>
		<li> Kaggle: M5 Forecasting - Uncertainty, Estimate the uncertainty distribution of Walmart unit sales. [<a href="https://www.kaggle.com/c/m5-forecasting-uncertainty"> link </a>]
		</li>
	</ul>

	<ul>[ Paper Replication ]
	<li> <B>Shihao Gu, Bryan Kelly and <a href="https://dachxiu.chicagobooth.edu/#research">Dacheng Xiu</a></B>
		<br> <B>"Empirical Asset Pricing via Machine Learning"</B>, Review of Financial Studies, Vol. 33, Issue 5, (2020), 2223-2273. Winner of the 2018 Swiss Finance Institute Outstanding Paper Award.
	 	<br> 
		[<a href="https://dachxiu.chicagobooth.edu/download/ML.pdf"> link </a>]
	<p>
	</li>	
	
	<li> <B>Jingwen Jiang, Bryan Kelly and <a href="https://dachxiu.chicagobooth.edu/#research">Dacheng Xiu</a></B>
		<br> <B>"(Re-)Imag(in)ing Price Trends"</B>, Chicago Booth Report, Aug 2021
	 	<br> 
		[<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3756587"> link </a>]
	<p>
	</li>	
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>


	
	<tr>
<td>11/19/2025, Wed</td>
<td>Lecture 11: Seminar and Student Presentations.
	<br>	
	<ul>[ Seminar ]
		<li><B>Title: Application of AI Technology in the Securities and Finance Industry</B>. [<a href=""> slides </a>]</li>  
		<li> <B>Speaker</B>: Dr. WANG Ying, Guosen Securities Co. Ltd. </li>
		<li> <B>Abstract</B>: 
			This talk focuses on AI applications in the securities and financial industry. First, we introduce the development of AI technologies. Second, we discuss the mainstream approaches to AI application in Fintech. Next, we will elaborate on Guosen's AI architecture design, including AI platform construction and core AI scenario applications, with a focus on how AI can help solve core challenges in these scenarios. Finally, we will explore the potential risks and future prospects of AI applications in finance.		
		</li>

		<li> <B>Bio</B>: <I>Dr. WANG Ying</I> holds a PhD in Computer Science and Engineering from the Hong Kong University of Science and Technology and a Bachelor's degree in Computer Science and Technology from the University of Science and Technology of China. She is currently an AI Architect at the Financial Technology Headquarters of Guosen Securities Co., Ltd., researching the application of AI in securities scenarios. Previously, during her doctoral studies at Huawei's 2012 Lab, her main research focus was on network resource allocation algorithms based on reinforcement learning.		</li>
	</ul>
	
	<ul>[ Selected Presentations ]
		<li>  Ming Mei, Li Xuzhi, Chen Yilin and Xie Xiaoxiao. Predicting Home Credit Default by Using Light Gradient Boosting Model. </li>
		<li>  Kuo Yang. MAFS5440 Project 1: Home Credit Default Risk [<a href=""> slides </a>]</li>
		<li>  Xinyu Xu, Zilong Pan and Hongen Tang. Home Credit Default Risk Assessment Based on LightGBMLinks to an external site. </li>
		<li>  Kedeng Qiu, Zewen Wan, Yifei Song and Jiarui Jiang. Project 1: Home Credit Default Risk. </li>
	</ul>	
</td>
<td>Y.Y.</td>
<td></td>
</tr>


<!----

	

	<tr>
<td>06/10/2024, Sun</td>
<td>Lecture 06: An Introduction to Convolutional Neural Networks [<a href="slides/Lecture06_CNN.pdf"> slides </a>]   
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> LeNet5 for MNIST dataset in Pytorch Notebook 
			[<a href="notebook/LeNet5_mnist.ipynb"> LeNet5_mnist.ipynb </a>] </li>
		<li> LeNet5 for Cifar10 dataset in Pytorch Notebook 
			[<a href="notebook/LeNet5_cifar10.ipynb"> LeNet5_cifar10.ipynb </a>] </li>
		<li> AlexNet for Cifar10 dataset in Pytorch Notebook 
			[<a href="notebook/AlexNet_cifar10.ipynb"> AlexNet_cifar10.ipynb </a>] </li>
		<li> ResNet for Cifar10 dataset in Pytorch Notebook 
			[<a href="notebook/ResNet_cifar10.ipynb"> ResNet_cifar10.ipynb </a>] </li>

	</ul>

	<ul>[ Topics on CNNs ]:
		<li> Visualizing CNNs, transfer learning, neural style and adversarial examples  [<a href="slides/Lecture07_CNN+.pdf"> slides </a>] </li>
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Visualization of VGG16 in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/vgg16-visualization.ipynb"> vgg16-visualization.ipynb </a>] </li>
		<li> Class activation heatmap of VGG16 in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/vgg16-heatmap.ipynb"> vgg16-heatmap.ipynb </a>] </li>
		<li> Fine-tuning (transfer learning) of ResNet in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/finetuning_resnet.ipynb"> finetuning_resnet.ipynb </a>] </li>
		<li> Neural Style of HKUST at Starry Night in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/neural_style_starry-hkust.ipynb"> neural_style_starry-hkust.ipynb </a>] </li>
		<li> Adversarial examples of LeNet5 with MNIST 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/LeNet5_mnist_fgsm.ipynb"> LeNet5_mnist_fgsm.ipynb </a>] </li>
	</ul>
	

</td>
<td>Y.Y.</td>
<td></td>
</tr>



	<td>09/10/2024, Wed </td>
<td>Lecture 07: Other nonlinear models moving beyond linearity <a href="slides/Lecture04_nonlinear.pdf">[ slides ]</a> 
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Python Notebook for ISLR Chapter 7 Lab 
			<a href="https://github.com/mscaudill/IntroStatLearn/blob/master/notebooks/Ch7_Moving_Beyond_Linearity/Ch7_Lab.ipynb"> [Ch7_Lab.ipynb ] </a> </li> 
	</li>
	</ul>

	<ul>[ Project 2 ]
		<li> Project description: paper replication study [<a href="project2/project2.pdf"> pdf </a>] </li>
		<li> Asset Pricing paper replication [<a href="project2/project2_Xiu_Asset_Pricing.pdf"> pdf </a>] </li>
		<li> Reimaging paper replication [<a href="project2/project2_Xiu_Reimage.pdf"> pdf </a>] </li>
	<br>	
	<li> <B>Shihao Gu, Bryan Kelly and <a href="https://dachxiu.chicagobooth.edu/#research">Dacheng Xiu</a></B>
		<br> <B>"Empirical Asset Pricing via Machine Learning"</B>, Review of Financial Studies, Vol. 33, Issue 5, (2020), 2223-2273. Winner of the 2018 Swiss Finance Institute Outstanding Paper Award.
	 	<br> 
		[<a href="https://dachxiu.chicagobooth.edu/download/ML.pdf"> link </a>]
	<p>
	</li>	
	
	<li> <B>Jingwen Jiang, Bryan Kelly and <a href="https://dachxiu.chicagobooth.edu/#research">Dacheng Xiu</a></B>
		<br> <B>"(Re-)Imag(in)ing Price Trends"</B>, Chicago Booth Report, Aug 2021
	 	<br> 
		[<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3756587"> link </a>]
	<p>
	</li>	
</td>
<td>Y.Y.</td>
<td></td>
</tr>

	<tr>
<td>16/10/2023, Wed</td>
<td>Lecture 08: An Introduction to Recurrent Neural Networks (RNN) and Long Short Term Memory (LSTM) [<a href="slides/Lecture08_RNN-LSTM.pdf"> slides </a>] </a>  
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Character-level RNN, LSTM and GRU for Name Classification 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/char_rnn_classification_tutorial.ipynb"> char_rnn_classification_tutorial.ipynb </a>] 
		</li>
		<li> RNN for generating Shakespeare's Sonnet 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/rnn.ipynb"> rnn.ipynb </a>] 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/shakespeare.txt"> shakespeare.txt </a>] 
		</li> 
		<li> LSTM for generating Shakespeare's Sonnet 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/rnn_lstm_shakespeare.ipynb"> rnn_lstm_shakespeare.ipynb </a>] 
		</li>	
		<li> Generating Shakespeare's Sonnet: RNN, LSTM, Bidirectional LSTM, and Momentum-LSTM
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/rnn_lstm_BiLSTM_mlstm_shakespeare.ipynb"> rnn_lstm_BiLSTM_mlstm_shakespeare.ipynb </a>]
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/rnn_lstm_biLSTM_shakespeare.ipynb"> rnn_lstm_biLSTM_shakespeare.ipynb </a>] 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/shakespeare.txt"> shakespeare.txt </a>]
		</li>
		<li> Bidirectional RNN for MNIST in pytorch
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/notebook/bidirection_lstm_mnist.ipynb"> bidirection_lstm_mnist.ipynb </a>] 
		[<a href="https://www.youtube.com/watch?v=jGst43P-TJA"> Youtube </a>]
		</li>	
		<p>
		<li> <a href="https://www.nobelprize.org/prizes/physics/2024/press-release/"> Nobel Prize in Physics 2024 </a> </li>
			<li> <a href="https://x.com/schmidhuberai/status/1844022724328394780?s=46&t=Eqe0JRFwCu11ghm5ZqO9xQ"> Jorgen Schmidhuber's Critique: </a>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">The <a href="https://twitter.com/hashtag/NobelPrizeinPhysics2024?src=hash&amp;ref_src=twsrc%5Etfw">#NobelPrizeinPhysics2024</a> for Hopfield &amp; Hinton rewards plagiarism and incorrect attribution in computer science. It&#39;s mostly about Amari&#39;s &quot;Hopfield network&quot; and the &quot;Boltzmann Machine.&quot; <br><br>1. The Lenz-Ising recurrent architecture with neuron-like elements was published in…</p>&mdash; Jürgen Schmidhuber (@SchmidhuberAI) <a href="https://twitter.com/SchmidhuberAI/status/1844022724328394780?ref_src=twsrc%5Etfw">October 9, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
			</li>
		<li> <a href="https://x.com/hardmaru/status/1253189802452647936"> Geoff Hinton's response to Schmidhuber's critique: </a> 
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Geoff Hinton&#39;s response to Schmidhuber&#39;s critique on r/ML <a href="https://t.co/kUIznzbRBh">https://t.co/kUIznzbRBh</a> <a href="https://t.co/joNcuzJnFk">https://t.co/joNcuzJnFk</a> <a href="https://t.co/3HWbfT5T9F">pic.twitter.com/3HWbfT5T9F</a></p>&mdash; hardmaru (@hardmaru) <a href="https://twitter.com/hardmaru/status/1253189802452647936?ref_src=twsrc%5Etfw">April 23, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>		
		</li>
		<li> Jorgen Schmidhuber: [<a href="https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html"> Deep Learning: Our Miraculous Year 1990-1991
 </a>] [<a href="https://people.idsia.ch/~juergen/critique-honda-prize-hinton.html"> Critique of Honda Prize for Dr. Hinton </a>]			
		</li>	  
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>

	<tr>
<td>23/10/2024, Wed</td>
<td>Lecture 09: Attention and Transformers [<a href="slides/Lecture09_transformer.pdf"> slides </a>]  </a>  
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Pytorch Twitter Sentiment Analysis: RNN, LSTM, BiLSTM, Multihead Self-Attention. 
		[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/capstone/2021.fall/notebook/PyTorch-RNN.ipynb"> RNN (ipynb) </a>] 
		[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/capstone/2021.fall/notebook/PyTorch-LSTM.ipynb"> LSTM (ipynb) </a>] 
		[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/capstone/2021.fall/notebook/PyTorch-BiLSTM.ipynb"> BiLSTM (ipynb) </a>] 
		[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/capstone/2021.fall/notebook/PyTorch-BiLSTM-with-MHSA.ipynb"> BiLSTM with Multihead Attention (ipynb) </a>] 	
		[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/capstone/2021.fall/notebook/PyTorch-BiLSTM-with-bertEmbedding.ipynb"> BERT embedding with BiLSTM (ipynb) </a>]
		[<a href="https://www.kaggle.com/kazanova/sentiment140"> Sentiment140 dataset </a>]
		</li>
		<li> Pytorch Sentiment Analysis with IMDB data: RNN, (bi)-LSTM, CNN, Transformer, BERT, etc. 
			[<a href="https://github.com/bentrevett/pytorch-sentiment-analysis"> GitHub </a>]
		</li>
		<li> Illustrated Transformer by Jay Alammar: [<a href="https://jalammar.github.io/illustrated-transformer/"> link </a>] </li>
		<li> The Annotated Transformer Tutorial by Sasha Rush: [<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html"> link </a>]
		</li>
		<li> BERT generation of Shakespeare's sonnet: [<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/data/BERT_shakespeare_gen.ipynb"> BERT_shakespeare_gen.ipynb </a>]
		</li>
		<li> BERT next sentence generation of Shakespeare's sonnet and Chinese poems: 
			[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/data/BERT_shakespeare_nextsen.ipynb"> BERT_shakespeare_nextsen.ipynb </a>]
		</li>
		<li> Chinese BERT (Whole-Word-Masking): [<a href="https://github.com/ymcui/Chinese-BERT-wwm"> link </a>]
		</li>
	</ul>
	<ul>[ Guest Talk ]
		<li> <B>Title</B>:  Sharing from Super Quantum Fund </li>
		<li> <B>Speaker</B>: Invited guests from <a href="https://en.superquant.fund/">Super Quantum</a> </li>
		<li> <B>Introduction</B>: Super Quantum is a quantitative asset management firm founded in 2019 by Professor Michael Zhang. With a vision to bring ‘genuine’ science to investing in the opportunistic China-A share market, 
			the team combines state-of-the-art research on deep learning, mathematics, econometrics and statistics, with solid experience in the finance industry to design quantitative strategies that consistently outperform the market. 
			Super Quantum currently manages 5-10 billion RMB and has Type 9 (asset management) license from SFC in Hong Kong. 
		</li>
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>

	<tr>
<td>30/10/2024, Wed</td>
<td>Lecture 10: Transformer and Applications [<a href="slides/Lecture09_transformer.pdf"> slides </a>]  </a>  
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Pytorch Twitter Sentiment Analysis: RNN, LSTM, BiLSTM, Multihead Self-Attention. 
		[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/capstone/2021.fall/notebook/PyTorch-RNN.ipynb"> RNN (ipynb) </a>] 
		[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/capstone/2021.fall/notebook/PyTorch-LSTM.ipynb"> LSTM (ipynb) </a>] 
		[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/capstone/2021.fall/notebook/PyTorch-BiLSTM.ipynb"> BiLSTM (ipynb) </a>] 
		[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/capstone/2021.fall/notebook/PyTorch-BiLSTM-with-MHSA.ipynb"> BiLSTM with Multihead Attention (ipynb) </a>] 	
		[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/capstone/2021.fall/notebook/PyTorch-BiLSTM-with-bertEmbedding.ipynb"> BERT embedding with BiLSTM (ipynb) </a>]
		[<a href="https://www.kaggle.com/kazanova/sentiment140"> Sentiment140 dataset </a>]
		</li>
		<li> Pytorch Sentiment Analysis with IMDB data: RNN, (bi)-LSTM, CNN, Transformer, BERT, etc. 
			[<a href="https://github.com/bentrevett/pytorch-sentiment-analysis"> GitHub </a>]
		</li>
		<li> Illustrated Transformer by Jay Alammar: [<a href="https://jalammar.github.io/illustrated-transformer/"> link </a>] </li>
		<li> The Annotated Transformer Tutorial by Sasha Rush: [<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html"> link </a>]
		</li>
		<li> BERT generation of Shakespeare's sonnet: [<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/data/BERT_shakespeare_gen.ipynb"> BERT_shakespeare_gen.ipynb </a>]
		</li>
		<li> BERT next sentence generation of Shakespeare's sonnet and Chinese poems: 
			[<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/data/BERT_shakespeare_nextsen.ipynb"> BERT_shakespeare_nextsen.ipynb </a>]
		</li>
		<li> Chinese BERT (Whole-Word-Masking): [<a href="https://github.com/ymcui/Chinese-BERT-wwm"> link </a>]
		</li>
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>



	<tr>
	<td>13/11/2024, Wed</td>
	<td>Lecture 12: Final Project. 
	<br>
	<ul>[ Project 3 ]		
		<li> Description: [<a href="project3/project3.pdf"> project3.pdf </a>]
		</li>
		<li> Kaggle: Jane Street Forecasting  [<a href="project3/project3.JS_forecast.pdf"> slides </a>]
		</li>
		<li> Kaggle: M5 Contests  [<a href="project3/project3.M5.pdf"> slides </a>]
		</li>
		<li> Cryptocurrency Trading Project [<a href="project3/project3-crypto%20trading.pdf"> slides (pdf) </a>]
		</li>	
		<li> Large Language Models with Financial Analysis [<a href="project3/project3-LLM%2Bfinance.pdf"> slides (pdf) </a>] </li>
		<p>
		</ul>
	
	<ul>[ Reference ]
		<li> Kaggle: Jane Street Forecasting. [<a href="https://www.kaggle.com/competitions/jane-street-real-time-market-data-forecasting/"> link </a>]
		</li>
		<li> Kaggle: G-Research Crypto Forecasting. [<a href="https://www.kaggle.com/c/g-research-crypto-forecasting/"> link </a>]
		</li>
		<li> Kaggle: M5 Forecasting - Accuracy, Estimate the unit sales of Walmart retail goods. [<a href="https://www.kaggle.com/c/m5-forecasting-accuracy"> link </a>]
		</li>
		<li> Kaggle: M5 Forecasting - Uncertainty, Estimate the uncertainty distribution of Walmart unit sales. [<a href="https://www.kaggle.com/c/m5-forecasting-uncertainty"> link </a>]
		</li>
	</ul>

	<ul>[ Reference: Guest Talk ]
		<li> <B>Title</B>:  Sequential Predictive Conformal Inference for Time Series [<a href="slides/CP_HKUST_Yao%2BChen.pdf"> slides </a>] </li> 
		<li> <B>Speaker</B>: <a href="https://sites.gatech.edu/chenxu97/">Chen XU</a>, Georgia Institute of Technology </li>
		<li> <B>Abstract</B>: We present a new distribution-free conformal prediction algorithm for sequential data (e.g., time series), called the sequential predictive conformal inference (SPCI). 
			We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction algorithms are not applicable. The main idea is to adaptively re-estimate the conditional quantile of non-conformity scores (e.g., prediction residuals), upon exploiting the temporal dependence among them. More precisely, we cast the problem of conformal prediction interval as predicting the quantile of a future residual, given a user-specified point prediction algorithm. Theoretically, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and real-data experiments, we demonstrate a significant reduction in interval width of SPCI compared to other existing methods under the desired empirical coverage. Source codes can be found at [<a href="https://github.com/hamrel-cxu/SPCI-code"> GitHub </a>].
		</li>
		<li> <B>Bio</B>: Chen Xu is currently a 4th year Operations Research PhD at Georgia Tech ISyE, where he is supervised by Prof. <a href="https://www2.isye.gatech.edu/~yxie77/">Yao Xie</a>. His current research interests are two-fold. (1) Uncertainty quantification for machine learning models. Specifically, advance conformal prediction as a distribution-free method for arbitrarily complex deep models, especially in the context of time-series modeling. (2) Generative models through flow-based neural networks. Specifically, develop scalable computational tools for problems at the intersection of statistics and optimization, including extensions to high-dimensional optimal transport, distributionally robust optimization, and differential privacy. He has published in top machine learning conferences (e.g., ICML 2021 oral, NeurIPS 2023 spotlight) and journals (e.g., IEEE TPAMI 2023, IEEE JSAIT). 
		</li>
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>


---->
	<!---
	<tr>
<td>16/10/2023, Mon</td>
<td>Lecture 06: Topics on CNN: Neural Style and Adversarial Examples [<a href="slides/Lecture07_CNN+.pdf"> slides </a>] and Support Vector Machines [<a href="slides/Lecture05_svm.pdf"> slides </a>]   
	<br>
	<ul>[ Topics on CNNs ]:
		<li> Visualizing CNNs [<a href="slides/Lecture07_CNN2.pdf"> YY's slides </a>] </li>
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Visualization of VGG16 in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/vgg16-visualization.ipynb"> vgg16-visualization.ipynb </a>] </li>
		<li> Class activation heatmap of VGG16 in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/vgg16-heatmap.ipynb"> vgg16-heatmap.ipynb </a>] </li>
		<li> Fine-tuning (transfer learning) of ResNet in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/finetuning_resnet.ipynb"> finetuning_resnet.ipynb </a>] </li>
		<li> Neural Style of HKUST at Starry Night in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/neural_style_starry-hkust.ipynb"> neural_style_starry-hkust.ipynb </a>] </li>
		<li> Adversarial examples of LeNet5 with MNIST 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/LeNet5_mnist_fgsm.ipynb"> LeNet5_mnist_fgsm.ipynb </a>] </li>
	</ul>

	<ul>[ SVM ]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> Python Notebook for Support Vector Machines 
			<a href="notebook/MAFS6010_svm.ipynb"> [ MAFS6010_svm.ipynb ] </a> </li> 
	</li>
	</ul>

	
	<ul>[ Project 2 ]
		<li> Project description: paper replication study [<a href="project2/project2.pdf"> pdf </a>] </li>
		<li> Asset Pricing paper replication [<a href="project2/replicate1_Xiu_Asset_Pricing.pdf"> pdf </a>] </li>
		<li> Reimaging paper replication [<a href="project2/replicate2_Xiu_Reimage.pdf"> pdf </a>] </li>
	<br>	
	<li> <B>Shihao Gu, Bryan Kelly and <a href="https://dachxiu.chicagobooth.edu/#research">Dacheng Xiu</a></B>
		<br> <B>"Empirical Asset Pricing via Machine Learning"</B>, Review of Financial Studies, Vol. 33, Issue 5, (2020), 2223-2273. Winner of the 2018 Swiss Finance Institute Outstanding Paper Award.
	 	<br> 
		[<a href="https://dachxiu.chicagobooth.edu/download/ML.pdf"> link </a>]
	<p>
	</li>	
	
	<li> <B>Jingwen Jiang, Bryan Kelly and <a href="https://dachxiu.chicagobooth.edu/#research">Dacheng Xiu</a></B>
		<br> <B>"(Re-)Imag(in)ing Price Trends"</B>, Chicago Booth Report, Aug 2021
	 	<br> 
		[<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3756587"> link </a>]
	<p>
	</li>	
	
		<li> GitHub Repository for reports of Project 2
		<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/project2/"> [ GitHub ] </a>
		<p>
		</li>	
		
	</ul>

	<ul>[Reading Material]:
	<li> <B>Shihao Gu, Bryan Kelly and <a href="https://dachxiu.chicagobooth.edu/#research">Dacheng Xiu</a></B>
		<br> <B>"Empirical Asset Pricing via Machine Learning"</B>, Review of Financial Studies, Vol. 33, Issue 5, (2020), 2223-2273. Winner of the 2018 Swiss Finance Institute Outstanding Paper Award.
	 	<br> 
		[<a href="https://dachxiu.chicagobooth.edu/download/ML.pdf"> link </a>]
	<p>
	</li>	
	<li> <B>Jingwen Jiang, Bryan Kelly and <a href="https://dachxiu.chicagobooth.edu/#research">Dacheng Xiu</a></B>
		<br> <B>"(Re-)Imag(in)ing Price Trends"</B>, Chicago Booth Report, Aug 2021
	 	<br> 
		[<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3756587"> link </a>]
	<p>
	</li>	
	<li> <B>Tracy Ke, Bryan Kelly and <a href="https://dachxiu.chicagobooth.edu/#research">Dacheng Xiu</a></B>
		<br> <B>"Predicting Returns with Text Data"</B>, Mar. 2022. Winner of the 2019 CICF Best Paper Award.
	 	<br> 
		[<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3389884"> link </a>]
	<p>
	</li>	
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>

	

	<tr>
	<td>20/11/2023, Mon</td>
<td>Lecture 10: Final Project and Talks. 
	<br>
	<ul>[ Project 3 ]		
		<li> Description: [<a href="project3/project3.pdf"> project3.pdf </a>]
		</li>
		<li> Kaggle Contests  [<a href="project3/project3_Crypto_M5.pdf"> slides </a>]
		</li>
		<li> Cryptocurrency Trading Project [<a href="project3/project3_CryptoTrading.pdf"> slides (pdf) </a>]
		</li>	
		<li> Large Language Models with Financial Analysis [<a href="project3/project3_LLM+finance.pdf"> slides (pdf) </a>] </li>
		<p>
		</ul>
	
	<ul>[ Reference ]
		<li> Kaggle: G-Research Crypto Forecasting. [<a href="https://www.kaggle.com/c/g-research-crypto-forecasting/"> link </a>]
		</li>
		<li> Kaggle: M5 Forecasting - Accuracy, Estimate the unit sales of Walmart retail goods. [<a href="https://www.kaggle.com/c/m5-forecasting-accuracy"> link </a>]
		</li>
		<li> Kaggle: M5 Forecasting - Uncertainty, Estimate the uncertainty distribution of Walmart unit sales. [<a href="https://www.kaggle.com/c/m5-forecasting-uncertainty"> link </a>]
		</li>
	</ul>

	<ul>[ Presentation ]
		<li> 1. <B> LI Aoran, MA Yijia, WENG Langting, ZHOU Tianying </B> [<I> Best Technique of Project 2 </I>]<img src="../../images/new.jpg" height="40">
	 	<br> 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/project2/best_tech_2_li_zhou_weng_ma.pdf"> report (pdf) </a>] [<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/project2/best_tech_2_li_zhou_weng_ma_presentation.pdf"> slides </a>]
	<p>
	</li>
	</ul>

	<ul>[ Guest Talk ]
		<li> <B>Title</B>:  Sequential Predictive Conformal Inference for Time Series [<a href="slides/CP_HKUST_Yao+Chen"> slides </a>] </li>
		<li> <B>Speaker</B>: <a href="https://sites.gatech.edu/chenxu97/">Chen XU</a>, Georgia Institute of Technology </li>
		<li> <B>Abstract</B>: We present a new distribution-free conformal prediction algorithm for sequential data (e.g., time series), called the sequential predictive conformal inference (SPCI). 
			We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction algorithms are not applicable. The main idea is to adaptively re-estimate the conditional quantile of non-conformity scores (e.g., prediction residuals), upon exploiting the temporal dependence among them. More precisely, we cast the problem of conformal prediction interval as predicting the quantile of a future residual, given a user-specified point prediction algorithm. Theoretically, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and real-data experiments, we demonstrate a significant reduction in interval width of SPCI compared to other existing methods under the desired empirical coverage. Source codes can be found at [<a href="https://github.com/hamrel-cxu/SPCI-code"> GitHub </a>].
		</li>
		<li> <B>Bio</B>: Chen Xu is currently a 4th year Operations Research PhD at Georgia Tech ISyE, where he is supervised by Prof. <a href="https://www2.isye.gatech.edu/~yxie77/">Yao Xie</a>. His current research interests are two-fold. (1) Uncertainty quantification for machine learning models. Specifically, advance conformal prediction as a distribution-free method for arbitrarily complex deep models, especially in the context of time-series modeling. (2) Generative models through flow-based neural networks. Specifically, develop scalable computational tools for problems at the intersection of statistics and optimization, including extensions to high-dimensional optimal transport, distributionally robust optimization, and differential privacy. He has published in top machine learning conferences (e.g., ICML 2021 oral, NeurIPS 2023 spotlight) and journals (e.g., IEEE TPAMI 2023, IEEE JSAIT). 
		</li>
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>


	<tr>
<td>27/11/2023, Mon</td>
<td>Lecture 11: Artificial Intelligence in Equity Investment  </a>  
	<br>
	<ul>[ Guest Talk ]
		<li> <B>Title</B>:  Artificial Intelligence in Equity Investment [<a href="slides/Lecture11_haifeng.pdf"> slides </a>] </li>
		<li> <B>Speaker</B>: Prof. <a href="http://www.bm.ust.hk/acct/faculty-and-staff/directory/achy">Haifeng You</a>, HKUST and Tsinghua University </li>
		<li> <B>Bio</B>: Haifeng You is a Professor of Accounting and Co-Director of the Center for Securities Analysis with Financial Technology at 
		Hong Kong University of Science and Technology (HKUST). His research focuses on the role of financial information and financial technology 
		in equity investment. He has published on leading academic and professional journals such as Journal of Financial Economics, Journal of 
		Accounting and Economics, and Financial Analyst Journal. Previously, he served as the Head of Quantitative Equity Research at China Investment 
		Corporation and Quantitative Researcher at Barclays Global Investors.  He has also served as advisors and consultants for investment firms 
		such as GSA capital in the UK and China Pacific Asset Managemen and Bosera Asset Management in China, helping them to build Asian and global 
		quantitative equity strategies. Professor You holds a PhD degree in Accounting from University of California, Berkeley and a BA degree in Finance from Peking University.
		</li>
	</ul>
	
	<ul>[ Presentation ]
		<li> 1. <B> HUANG Yuxin, LEI Yunxin, AN Tianyuan, LIN Fengshan, LIU Zongxuan </B> [<I> Best Writing of Project 2 </I>]<img src="../../images/new.jpg" height="40">
	 	<br> 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/tree/master/project2/best_writing_1_huang_lei_an_lin_liu.pdf"> report (pdf) </a>] [<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/tree/master/project2/best_writing_1_Presentation.pptx"> slides (pptx) </a>]
	<p>
	</li>
		<li> 2. <B> JIA Yaoyao, JIANG Xiaoyue, YANG Tianhao, HUANG Yuxuan </B> [<I> Best Report of Project 2 </I>]<img src="../../images/new.jpg" height="40">
	 	<br> 
		[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/tree/master/project2/best_overall_1_yang_jia_jiang_huang.pdf"> report (pdf) </a>] [<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/tree/master/project2/best_overall_1_yang_jia_jiang_huang_Presentation.pptx"> slides (pptx) </a>]
	<p>
	</li>
	</ul>

</td>
<td>Y.Y.</td>
<td></td>
</tr>


	<tr>
<td>25/11/2021, Thu</td>
<td>Lecture 12: An Introduction to Unsupervised Learning: PCA, AutoEncoder, VAE, and GANs [<a href="slides/Lecture11_GAN.pdf"> slides </a>] </a>  
	<br>
	<ul>[Reference]:
		<li> To view .ipynb files below, you may try <a href="https://nbviewer.jupyter.org/"> [ Jupyter NBViewer] </a> </li> 
		<li> DCGAN for MNIST Tutorial in Pytorch Notebook 
			[<a href="https://github.com/aifin-hkust/aifin-hkust.github.io/blob/master/2020/notebook/dcgan_mnist_tutorial.ipynb"> dcgan_mnist_tutorial.ipynb </a>] 
		</li>
		<p>
			
		<li> Credit Card Fraud Detection via GAN implemented by Ruoxue LIU: [<a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/aifin/2021/notebook/gan_creditcard_augmentation_11_23_2/simple_gan.ipynb"> GitHub </a>]
		</li>
		<li> Credit Card Fraud Detection dataset: [<a href="https://www.kaggle.com/mlg-ulb/creditcardfraud/home"> Kaggle </a>]	
		</li>
		<p>
		<li> <a href="https://github.com/yao-lab/yao-lab.github.io/blob/master/data/RobustGAN/Robust_GAN.ipynb"> Robust_GAN.ipynb </a>: Jupyter Notebook for demonstration </li>
		<li> <a href="https://github.com/yao-lab/Robust-GAN-Center"> Robust-GAN-Center </a>: robust center (mean) estimate via GANs </li>
		<li> <a href="https://github.com/zhuwzh/Robust-GAN-Scatter"> Robust-GAN-Scatter </a>: robust scatter (covariance) estimate via GANs </li>
		<p>
		<li>  <B> GAO, Chao, Jiyi LIU, Yuan YAO, and Weizhi ZHU.</B>
			<br>
			Robust Estimation and Generative Adversarial Nets. 
			<br>
			<I>ICLR</I> 2019.
			<br>
			[<a href="https://arxiv.org/abs/1810.02030"> arXiv:1810.02030 </a>] [<a href="https://github.com/zhuwzh/Robust-GAN-Center"> GitHub </a>] [<a href="https://simons.berkeley.edu/talks/robust-estimation-and-generative-adversarial-nets"> GAO, Chao's Simons Talk </a>]
		</li>
		<p>
		<li> <B>GAO, Chao, Yuan YAO, and Weizhi ZHU.</B>
			<br>
			Generative Adversarial Nets for Robust Scatter Estimation: A Proper Scoring Rule Perspective. 
			<br>
			<I>Journal of Machine Learning Research</I>, 21(160):1-48, 2020. 
			<br>
			[<a href="https://arxiv.org/abs/1903.01944"> arXiv:1903.01944 </a>] [<a href="https://github.com/zhuwzh/Robust-GAN-Scatter"> GitHub </a>]
		</li>
	</ul>
</td>
<td>Y.Y.</td>
<td></td>
</tr>
	
--->

	<!---
	<ul>[ Guest Talk ]
		<li> Title: An Overview of Quant Modeling and Trading: Theory and Practice [<a href="slides/Lecture01_quant_AI_overview_ust.pdf"> slides </a>] </li>
		<li> Speaker: Prof. <a href="https://mikezhang.com"> Michael Zhang</a>, CUHK </li>
		<li> Bio: Professor Michael Zhang is the Irwin and Joan Jacobs Chair Professor at the School of Economics and Management, Tsinghua University. 
		He was previously at the Chinese University of Hong Kong, where he served as the Associate Dean of Innovation and Impact, Co-Executive Director 
		of Asia Pacific Institute of Business, Co-Director of Hong Kong-Shenzhen Finance Research Centre at the CUHK Business School. Externally, he is also 
		affiliated with MIT Initiative on Digital Economy and Leibniz Centre for European Economic Research (ZEW).
			
                     He has a PhD in Management from MIT Sloan School of Management, an MSc in Management, a BE in Computer Science and a BA in English 
		from Tsinghua University. Before joining the academia, he worked as an analyst for an investment bank, and as an international marketing manager 
			for a high-tech company. He holds a US patent, and cofounded several companies in Social Networking (Unknown Space, MITBBS, the biggest 
			social network for Chinese in America) and FinTech (Super Quantum Fund). 
		</li>
	</ul>	
	--->
	
</tbody>
</table>



<hr>

<address>
by <a href="http://yao-lab.github.io/">YAO, Yuan</a>.
</address>

</body>
</html>





